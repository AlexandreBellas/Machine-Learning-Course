{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTlpjsyHUTv-"
   },
   "source": [
    "# Aula 06 - Exercício 05\n",
    "\n",
    "### Alunos:\n",
    " - Alexandre Batistella Bellas, 9763168\n",
    " - Moisés Botarro Ferraz Silva, 8504135"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lj8Koo4pUWWU"
   },
   "source": [
    "## Multilayer Perceptron para classificação do dataset Wine\n",
    "Neste exercício utilizaremos a base de dados Wine, que consiste em 178 exemplos de dimensionalidade 13, separados em 3 classes. Os atributos dessa base de dados não estão normalizados, apresentando intervalos variados de valores.\n",
    "\n",
    "- Carregue a base de dados wine e normalize entre 0 e 1 o valor dos seus atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qgP1cqVAUMQc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- original values ---\n",
      "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
      "--- normalized values ---\n",
      "[[0.84210526 0.1916996  0.57219251 ... 0.45528455 0.97069597 0.56134094]\n",
      " [0.57105263 0.2055336  0.4171123  ... 0.46341463 0.78021978 0.55064194]\n",
      " [0.56052632 0.3201581  0.70053476 ... 0.44715447 0.6959707  0.64693295]\n",
      " ...\n",
      " [0.58947368 0.69960474 0.48128342 ... 0.08943089 0.10622711 0.39728959]\n",
      " [0.56315789 0.36561265 0.54010695 ... 0.09756098 0.12820513 0.40085592]\n",
      " [0.81578947 0.66403162 0.73796791 ... 0.10569106 0.12087912 0.20114123]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = load_wine()\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "print(\"--- original values ---\\n%s\" % data.data)\n",
    "data.data = scaler.fit_transform(data.data)\n",
    "print(\"--- normalized values ---\\n%s\" % data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxuLjxtg0uZ2"
   },
   "source": [
    "---\n",
    "- Agora divida a base em conjunto de teste e treino. Utilize 20% da base para teste e 80% para treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8T1WrYHVs80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set: 142\n",
      "size of testing set: 36\n",
      "20.22% of data are used for test\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, shuffle=True, random_state=10)\n",
    "print(\"size of training set: %s\" % len(x_train))\n",
    "print(\"size of testing set: %s\" % len(x_test))\n",
    "print(\"%.2f%% of data are used for test\" % (len(x_test)*100/(len(data.data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YpZ85fjn08Hp"
   },
   "source": [
    "---\n",
    "- Treine 3 classificadores MLP (com `solver='sgd'`), variando o número de neurônios na camada escondida. Reporte o score dos classificadores nos conjuntos de treino e teste.\n",
    "  - Parâmetros para  alterar: hidden_layer_sizes=(25,), max_iter=1000,\n",
    "                      solver='sgd', tol=1e-4, random_state=1,\n",
    "                      learning_rate_init=.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6B5F6mAazFO9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1 neuron in the hidden layer ---\n",
      "predicted [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] \n",
      "wanted    [1 1 0 1 0 1 1 0 2 0 0 1 0 1 1 1 1 1 1 2 0 2 0 0 1 2 1 2 1 1 2 2 1 2 1 0] \n",
      "accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(1,),solver='sgd', tol=1e-4, random_state=1, learning_rate_init=.1)\n",
    "mlp1.fit(x_train, y_train)\n",
    "\n",
    "print(\"--- 1 neuron in the hidden layer ---\")\n",
    "print(\"predicted %s \" % mlp1.predict(x_test))\n",
    "print(\"wanted    %s \" % y_test)\n",
    "print(\"accuracy: %s\" % mlp1.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3 neuron in the hidden layer ---\n",
      "predicted [1 1 0 1 0 1 1 0 2 0 0 1 0 2 1 1 2 1 1 2 0 2 0 0 1 2 2 2 1 1 2 2 1 2 1 0] \n",
      "wanted    [1 1 0 1 0 1 1 0 2 0 0 1 0 1 1 1 1 1 1 2 0 2 0 0 1 2 1 2 1 1 2 2 1 2 1 0] \n",
      "accuracy: 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "mlp3 = MLPClassifier(hidden_layer_sizes=(3,),solver='sgd', tol=1e-4, random_state=1, learning_rate_init=.1)\n",
    "mlp3.fit(x_train, y_train)\n",
    "\n",
    "print(\"--- 3 neuron in the hidden layer ---\")\n",
    "print(\"predicted %s \" % mlp3.predict(x_test))\n",
    "print(\"wanted    %s \" % y_test)\n",
    "print(\"accuracy: %s\" % mlp3.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10 neuron in the hidden layer ---\n",
      "predicted [1 1 0 1 0 1 1 0 2 0 0 1 0 1 1 1 2 1 1 2 0 2 0 0 1 2 0 2 1 1 2 2 1 2 1 0] \n",
      "wanted    [1 1 0 1 0 1 1 0 2 0 0 1 0 1 1 1 1 1 1 2 0 2 0 0 1 2 1 2 1 1 2 2 1 2 1 0] \n",
      "accuracy: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "mlp10 = MLPClassifier(hidden_layer_sizes=(10,),solver='sgd', tol=1e-4, random_state=1, learning_rate_init=.1)\n",
    "mlp10.fit(x_train, y_train)\n",
    "\n",
    "print(\"--- 10 neuron in the hidden layer ---\")\n",
    "print(\"predicted %s \" % mlp10.predict(x_test))\n",
    "print(\"wanted    %s \" % y_test)\n",
    "print(\"accuracy: %s\" % mlp10.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PfX6WYjh13GS"
   },
   "source": [
    "---\n",
    "- Selecione uma amostra de tamanho 10 do conjunto de testes. Selecione o classificador que apresentou maior acurácia no conjunto de testes e calcule, para cada elemento da amostra, a classe esperada, a classe obtida e a probabilidade estimada de cada classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O classificador que apresentou melhor acurácia foi aquele com 10 neurônios na camada intermediária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V98MdpUaJPuH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wanted:    [0 1 0 1 1 1 1 1 1 2]\n",
      "predicted: [0 1 0 1 1 1 2 1 1 2]\n",
      "accuracy:  0.9\n",
      "\n",
      "--- predicted probabilities ---\n",
      "ID \tWANTED \tPREDICT\tCLASS 0 \tCLASS 1\t \tCLASS 2\n",
      "0 \t0 \t0 \t0.9997354555\t0.0002263466\t0.0000381979\n",
      "1 \t1 \t1 \t0.0097434700\t0.9902552668\t0.0000012631\n",
      "2 \t0 \t0 \t0.9993213421\t0.0006094791\t0.0000691789\n",
      "3 \t1 \t1 \t0.0074663873\t0.6566399478\t0.3358936649\n",
      "4 \t1 \t1 \t0.0127505262\t0.9871943412\t0.0000551326\n",
      "5 \t1 \t1 \t0.0008270471\t0.9990690221\t0.0001039308\n",
      "6 \t1 \t2 \t0.0028499246\t0.2378374667\t0.7593126087\n",
      "7 \t1 \t1 \t0.0148901441\t0.9849755791\t0.0001342767\n",
      "8 \t1 \t1 \t0.0000668693\t0.9999330333\t0.0000000974\n",
      "9 \t2 \t2 \t0.0018514388\t0.0003945606\t0.9977540006\n"
     ]
    }
   ],
   "source": [
    "# Vamos pegar um conjunto de amostras de tamanho 10 que inclua o exemplo classificado erroneamento para mpl10.\n",
    "\n",
    "x_test_10 = x_test[10:20]\n",
    "y_test_10 = y_test[10:20]\n",
    "\n",
    "predicted = mlp10.predict(x_test_10)\n",
    "prob = mlp10.predict_proba(x_test_10)\n",
    "\n",
    "print(\"wanted:    %s\" % y_test_10)\n",
    "print(\"predicted: %s\" % predicted)\n",
    "print(\"accuracy:  %s\\n\" % mlp10.score(x_test_10, y_test_10))\n",
    "\n",
    "print(\"--- predicted probabilities ---\")\n",
    "print(\"ID \\tWANTED \\tPREDICT\\tCLASS 0 \\tCLASS 1\\t \\tCLASS 2\")\n",
    "for (el,score) in zip(range(0,len(prob)),prob):\n",
    "    print(\"%i \\t%i \\t%i \\t%.10f\\t%.10f\\t%.10f\" % (el, y_test_10[el], predicted[el], score[0], score[1], score[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "03NDi1Re23W5"
   },
   "source": [
    "---\n",
    "- Agora sobre a mesma amostra escolhida anteriormente, selecione o classificador que apresentou menor acurácia no conjunto de testes e calcule, para cada elemento da amostra, a classe esperada, a classe obtida e a probabilidade esperada de cada classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5OBeWOR3Kdr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wanted:    [0 1 0 1 1 1 1 1 1 2]\n",
      "predicted: [1 1 1 1 1 1 1 1 1 1]\n",
      "accuracy:  0.7\n",
      "\n",
      "--- predicted probabilities ---\n",
      "ID \tWANTED \tPREDICT\tCLASS 0 \tCLASS 1\t \tCLASS 2\n",
      "0 \t0 \t1 \t0.3042266611\t0.4074168246\t0.2883565143\n",
      "1 \t1 \t1 \t0.3042266611\t0.4074168246\t0.2883565143\n",
      "2 \t0 \t1 \t0.3042266611\t0.4074168246\t0.2883565143\n",
      "3 \t1 \t1 \t0.3042266611\t0.4074168246\t0.2883565143\n",
      "4 \t1 \t1 \t0.3042266611\t0.4074168246\t0.2883565143\n",
      "5 \t1 \t1 \t0.2805607042\t0.4304869619\t0.2889523339\n",
      "6 \t1 \t1 \t0.3042266611\t0.4074168246\t0.2883565143\n",
      "7 \t1 \t1 \t0.3042266611\t0.4074168246\t0.2883565143\n",
      "8 \t1 \t1 \t0.3042266611\t0.4074168246\t0.2883565143\n",
      "9 \t2 \t1 \t0.3042266611\t0.4074168246\t0.2883565143\n"
     ]
    }
   ],
   "source": [
    "predicted = mlp1.predict(x_test_10)\n",
    "prob = mlp1.predict_proba(x_test_10)\n",
    "\n",
    "print(\"wanted:    %s\" % y_test_10)\n",
    "print(\"predicted: %s\" % predicted)\n",
    "print(\"accuracy:  %s\\n\" % mlp1.score(x_test_10, y_test_10))\n",
    "\n",
    "print(\"--- predicted probabilities ---\")\n",
    "print(\"ID \\tWANTED \\tPREDICT\\tCLASS 0 \\tCLASS 1\\t \\tCLASS 2\")\n",
    "for (el,score) in zip(range(0,len(prob)),prob):\n",
    "    print(\"%i \\t%i \\t%i \\t%.10f\\t%.10f\\t%.10f\" % (el, y_test_10[el], predicted[el], score[0], score[1], score[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZF6M90l3dfd"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "- Faça uma análise das probabilidades calculadas por cada classificador nos exemplos corretos e relacione-as com a 'qualidade' dos mesmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCVPzVVJ7wql"
   },
   "source": [
    "No caso do classificador com 10 neurônios na camada intermediária, a maioria dos exemplos foi classificado na classe correta com uma certeza acima de 98%. A exceção está na amostra 3 que, apesar de ser classificada corretamente, houve uma maior incerteza na classificação, refletida pelos valores de probabilidade (65.66% para classe 1 x 33.59% para classe 2). Uma amostra (id 6) foi classificada erroneamente mas também com uma maior incerteza associada (75.93% ao invés dos altos valores para as demais amostras).\n",
    "\n",
    "Para o classificador com apenas 1 neurônio na camada intermediária, 3 amostras foram classificadas de maneira errada. Entretanto, mesmo as classificadas corretamente apresentam um elevado grau de incerteza na classificação. Repare que as probabilidades de pertencer às classes 0, 1 e 2 são basicamente as mesmas para todos os exemplos, o que indica que apenas um neurônio não foi capaz de capturar informações suficientes para diferenciar as classes.\n",
    "\n",
    "Podemos concluir, dessa forma, que mesmo para as amostras classificadas corretamente pelos dois modelos, o com 10 neurônios possui uma maior certeza nessa classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BkCZAX_VUdDD"
   },
   "source": [
    "---\n",
    "- Calcule, usando 10-fold cross-validation, a acurácia média da melhor configuração de classificador que você utilizou e seu desvio padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lJCpQ8FU8ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies: [1.0, 0.9444444444444444, 1.0, 0.9444444444444444, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "mean:       0.9888888888888889\n",
      "std :       0.022222222222222233\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "X = data.data\n",
    "Y = data.target\n",
    "acs = []\n",
    "\n",
    "for train_index, test_index in skf.split(X,Y):\n",
    "    x_train, x_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    acs.append(mlp10.score(x_test, y_test))\n",
    "    \n",
    "mean, std = np.mean(acs), np.std(acs)\n",
    "print(\"accuracies: %s\" % acs)\n",
    "print(\"mean:       %s\" % mean)\n",
    "print(\"std :       %s\" % std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando os valores de acurácia, é possível concluir que a cross validation é um melhor método para avaliar a performance do nosso modelo. Quando treinamos o MLP para 10 neurônios, usamos apenas um conjunto de teste. Caso esse conjunto de teste fosse um dos quais a acurácia é igual à 1, poderíamos achar que nosso modelo estava com alta performance. Entretanto, utilizando outros elementos para o conjunto de testes, chegaríamos à uma acurácia menor. \n",
    "\n",
    "Aplicando a Stratified K Fold, obtemos uma avaliação mais precisa para o nosso modelo uma vez que variamos o nosso conjunto de testes, o que permite treinar melhor o modelo e chegar a um resultado final mais assertivo. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Aula06-Exercicio05.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
